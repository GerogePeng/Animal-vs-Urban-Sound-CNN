{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed0c7bee-3b80-4956-bf30-0ab5838656fb",
   "metadata": {},
   "source": [
    "# Detect Animal and UrbanSound"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c571834-c72d-45ae-a509-25e47a797dfe",
   "metadata": {},
   "source": [
    "## Packages and Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e91dfc0-13eb-4d56-a996-b79389a8fb17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa\n",
    "import librosa.display\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, regularizers\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, precision_recall_fscore_support\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import seaborn as sns\n",
    "\n",
    "# 設定參數\n",
    "BASE_PATH = r\"~/data\"\n",
    "SAMPLE_RATE = 44100\n",
    "MAX_DURATION = 5\n",
    "MAX_SAMPLES = SAMPLE_RATE * MAX_DURATION\n",
    "N_MELS = 32\n",
    "N_FFT = 256\n",
    "HOP_LENGTH = 64\n",
    "N_CLASSES = 2\n",
    "CLASS_MAPPING = {\n",
    "    \"Animals\": 0,\n",
    "    \"urban noises\": 1\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b40f91dd-9df1-4a00-b4da-51c3f4e1e499",
   "metadata": {},
   "source": [
    "## melspectrogram and split data\n",
    "- extract_melspectrogram \n",
    "- shuffled randomly and split train(augment), validation, test data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a21eb9-0be4-4b0d-a919-ac85b5addb30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_melspectrogram(file_path, augment=False):\n",
    "    try:\n",
    "        audio, sr = librosa.load(file_path, sr=SAMPLE_RATE)\n",
    "\n",
    "        # augment\n",
    "        if augment:\n",
    "            # Add noise\n",
    "            if np.random.random() < 0.3:\n",
    "                noise = np.random.normal(0, 0.005, audio.shape)\n",
    "                audio = audio + noise\n",
    "            \n",
    "            # Time shift \n",
    "            if np.random.random() < 0.3:\n",
    "                shift = int(np.random.uniform(-0.2, 0.2) * len(audio))\n",
    "                audio = np.roll(audio, shift)\n",
    "            \n",
    "            # Volume adjustment\n",
    "            if np.random.random() < 0.3:\n",
    "                audio = audio * np.random.uniform(0.8, 1.2)\n",
    "\n",
    "        if len(audio) != MAX_SAMPLES:\n",
    "            print(f\"Skipping {file_path}: audio length {len(audio)} does not match required length {MAX_SAMPLES}\")\n",
    "            return None\n",
    "        \n",
    "        \n",
    "        mel_spec = librosa.feature.melspectrogram(\n",
    "            y=audio, \n",
    "            sr=sr, \n",
    "            n_fft=N_FFT, \n",
    "            hop_length=HOP_LENGTH, \n",
    "            n_mels=N_MELS,\n",
    "        )\n",
    "        \n",
    "        mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)      \n",
    "\n",
    "        mel_spec_db = (mel_spec_db - np.mean(mel_spec_db)) / (np.std(mel_spec_db) + 1e-8)\n",
    "        \n",
    "        return mel_spec_db\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# load data and subset\n",
    "def load_data_subset_(file_list, augment_data):\n",
    "    X_subset = []\n",
    "    y_subset = []\n",
    "    file_paths_subset = []\n",
    "    for file_path, class_label in file_list:\n",
    "        #  augment_data = True --> augment\n",
    "        mel_spec = extract_melspectrogram(file_path, augment=augment_data)\n",
    "        if mel_spec is not None:\n",
    "            X_subset.append(mel_spec)\n",
    "            y_subset.append(class_label)\n",
    "            file_paths_subset.append(file_path)\n",
    "    return np.array(X_subset), np.array(y_subset), np.array(file_paths_subset)\n",
    "\n",
    "\n",
    "# load_data and split train, validation, test data \n",
    "def load_data(data_dir, random_seed=507):\n",
    "    import random\n",
    "    from sklearn.utils import shuffle\n",
    "    \n",
    "    np.random.seed(random_seed)\n",
    "    random.seed(random_seed)    \n",
    "  \n",
    "    # file path and label \n",
    "    all_files = []\n",
    "    for class_name in os.listdir(data_dir):\n",
    "        class_path = os.path.join(data_dir, class_name)\n",
    "        \n",
    "        if not os.path.isdir(class_path) or class_name not in CLASS_MAPPING:\n",
    "            continue\n",
    "        \n",
    "        class_label = CLASS_MAPPING[class_name]\n",
    "        \n",
    "        for file_name in os.listdir(class_path):\n",
    "            if file_name.endswith('.wav'):\n",
    "                file_path = os.path.join(class_path, file_name)\n",
    "                all_files.append((file_path, class_label))\n",
    "    \n",
    "    # shuffled randomly\n",
    "    random.shuffle(all_files)\n",
    "    print(f\"Found {len(all_files)} audio files, shuffled randomly\")\n",
    "    \n",
    "    train_val_files, test_files = train_test_split(\n",
    "        all_files, test_size=0.1, random_state=random_seed)\n",
    "    \n",
    "    train_files, val_files = train_test_split(\n",
    "        train_val_files, test_size=(0.2 / 0.9), random_state=random_seed)\n",
    "    \n",
    "    # spilt data \n",
    "    print(\"\\nProcessing training data (with augmentation)...\")\n",
    "    X_train, y_train, file_paths_train = load_data_subset_(train_files, augment_data=True)\n",
    "    print(\"Processing validation data (without augmentation)...\")\n",
    "    X_val, y_val, file_paths_val = load_data_subset_(val_files, augment_data=False)\n",
    "    print(\"Processing test data (without augmentation)...\")\n",
    "    X_test, y_test, file_paths_test = load_data_subset_(test_files, augment_data=False)\n",
    "    \n",
    "    # shuffle*2 \n",
    "    X_train, y_train, file_paths_train = shuffle(X_train, y_train, file_paths_train, random_state=random_seed)\n",
    "    X_val, y_val, file_paths_val = shuffle(X_val, y_val, file_paths_val, random_state=random_seed)\n",
    "    X_test, y_test, file_paths_test = shuffle(X_test, y_test, file_paths_test, random_state=random_seed)\n",
    "    \n",
    "    # Add a dimension (CNN)\n",
    "    X_train = X_train[..., np.newaxis]\n",
    "    X_val = X_val[..., np.newaxis]\n",
    "    X_test = X_test[..., np.newaxis]\n",
    "    \n",
    "    # Display the category distribution of each data set\n",
    "    print(\"\\n--- Class Distributions ---\")\n",
    "    unique_train, counts_train = np.unique(y_train, return_counts=True)\n",
    "    print(f\"Train set class distribution: {dict(zip(unique_train, counts_train))}\")\n",
    "    unique_val, counts_val = np.unique(y_val, return_counts=True)\n",
    "    print(f\"Validation set class distribution: {dict(zip(unique_val, counts_val))}\")\n",
    "    unique_test, counts_test = np.unique(y_test, return_counts=True)\n",
    "    print(f\"Test set class distribution: {dict(zip(unique_test, counts_test))}\")\n",
    "\n",
    "    return (X_train, y_train, file_paths_train,\n",
    "            X_val, y_val, file_paths_val,\n",
    "            X_test, y_test, file_paths_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230dc29a-a288-4469-ae5d-c49a630f3436",
   "metadata": {},
   "source": [
    "## Balanced Batch Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e2e32a9-508f-4cf2-960f-0cb270cea0f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BalancedBatchGenerator(tf.keras.utils.Sequence):\n",
    "    def __init__(self, X, y, batch_size=32, shuffle=True):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        \n",
    "        # Separate different categories \n",
    "        self.class_0_indices = np.where(y == 0)[0]\n",
    "        self.class_1_indices = np.where(y == 1)[0]\n",
    "        \n",
    "        # The number of samples in each category in the batch\n",
    "        self.samples_per_class = batch_size\n",
    "        \n",
    "        # Calculate the total batch size (based on the fewer categories)\n",
    "        min_samples = min(len(self.class_0_indices), len(self.class_1_indices))\n",
    "        self.n_batches = min_samples // self.samples_per_class\n",
    "        \n",
    "        print(f\"Balanced batch generator created:\")\n",
    "        print(f\"  Class 0 samples: {len(self.class_0_indices)}\")\n",
    "        print(f\"  Class 1 samples: {len(self.class_1_indices)}\")\n",
    "        print(f\"  Samples per class per batch: {self.samples_per_class}\")\n",
    "        print(f\"  Total batches: {self.n_batches}\")\n",
    "        \n",
    "        # shuffled index\n",
    "        self.shuffled_class_0_indices = np.random.permutation(self.class_0_indices)\n",
    "        self.shuffled_class_1_indices = np.random.permutation(self.class_1_indices)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.n_batches\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        # Take samples from each category\n",
    "        start_idx = index * self.samples_per_class\n",
    "        end_idx = (index + 1) * self.samples_per_class\n",
    "        \n",
    "        batch_indices_0 = self.shuffled_class_0_indices[start_idx:end_idx]\n",
    "        batch_indices_1 = self.shuffled_class_1_indices[start_idx:end_idx]\n",
    "        \n",
    "        # concatenate and shuffle\n",
    "        batch_indices = np.concatenate([batch_indices_0, batch_indices_1])\n",
    "        np.random.shuffle(batch_indices)\n",
    "        \n",
    "        return self.X[batch_indices], self.y[batch_indices]\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle:\n",
    "            self.shuffled_class_0_indices = np.random.permutation(self.class_0_indices)\n",
    "            self.shuffled_class_1_indices = np.random.permutation(self.class_1_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27a1b84-abea-4ae0-b738-85a47a08ea7e",
   "metadata": {},
   "source": [
    "## Build model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c765108-7fa8-44f5-9f1a-2ce47d89d4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(input_shape):\n",
    "    model = models.Sequential([\n",
    "        # 1 convolutional block\n",
    "        layers.Conv2D(64, (3, 3), activation='relu', padding='same', \n",
    "                     input_shape=input_shape,\n",
    "                     kernel_regularizer=regularizers.l2(0.001)),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Conv2D(64, (3, 3), activation='relu', padding='same',\n",
    "                     kernel_regularizer=regularizers.l2(0.001)),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Dropout(0.25),\n",
    "        \n",
    "        # 2 convolutional block\n",
    "        layers.Conv2D(128, (3, 3), activation='relu', padding='same',\n",
    "                     kernel_regularizer=regularizers.l2(0.001)),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Conv2D(64, (3, 3), activation='relu', padding='same',\n",
    "                     kernel_regularizer=regularizers.l2(0.001)),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Dropout(0.25), \n",
    "        \n",
    "        # Dense\n",
    "        layers.GlobalAveragePooling2D(),  \n",
    "        layers.Dense(64, activation='relu',\n",
    "                    kernel_regularizer=regularizers.l2(0.001)),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(32, activation='relu',\n",
    "                    kernel_regularizer=regularizers.l2(0.001)),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(N_CLASSES, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    # model.compile\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f51759-e582-474d-b355-97d124202fb7",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0f9542-5fbf-4a8d-81a3-07f368e6644f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model():\n",
    "    # GPU \n",
    "    physical_devices = tf.config.list_physical_devices('GPU')\n",
    "    if len(physical_devices) > 0:\n",
    "        tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "    \n",
    "    print(\"Loading data ...\")\n",
    "    X_train, y_train, file_paths_train, X_val, y_val, file_paths_val,X_test, y_test, file_paths_test= load_data(BASE_PATH)\n",
    "            \n",
    "    \n",
    "    # Batch Generator\n",
    "    train_generator = BalancedBatchGenerator(X_train, y_train, batch_size=5, shuffle=True)\n",
    "    val_generator = BalancedBatchGenerator(X_val, y_val, batch_size=5, shuffle=False)\n",
    "    \n",
    "    # compute class weight\n",
    "    class_weights = compute_class_weight('balanced', \n",
    "                                       classes=np.unique(y_train), \n",
    "                                       y=y_train)\n",
    "    class_weight_dict = dict(enumerate(class_weights))\n",
    "    print(f\"Class weights: {class_weight_dict}\")\n",
    "    \n",
    "    # build model\n",
    "    input_shape = X_train.shape[1:]\n",
    "    model = build_model(input_shape)\n",
    "    model.summary()\n",
    "    \n",
    "    # Callback Function\n",
    "    callbacks = [\n",
    "        ModelCheckpoint('best_audio_model_improved.h5', \n",
    "                       monitor='val_loss', \n",
    "                       mode='min', \n",
    "                       save_best_only=True, \n",
    "                       verbose=1,\n",
    "                       save_weights_only=False),\n",
    "        EarlyStopping(monitor='val_loss', \n",
    "                     patience=10, \n",
    "                     restore_best_weights=True, \n",
    "                     verbose=1),\n",
    "        ReduceLROnPlateau(monitor='val_loss', \n",
    "                         factor=0.5, \n",
    "                         patience=7, \n",
    "                         min_lr=1e-7, \n",
    "                         verbose=1)\n",
    "    ]\n",
    "    \n",
    "    history = model.fit(\n",
    "        train_generator,\n",
    "        epochs=30,\n",
    "        validation_data=val_generator,\n",
    "        callbacks=callbacks,\n",
    "        class_weight=class_weight_dict,  # class weight\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    return model, history, X_test, y_test, file_paths_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a09f82-58a8-4cad-a5a2-0e30ed48431f",
   "metadata": {},
   "source": [
    "## Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457a71dd-a874-47c6-983e-3b7f69605707",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_test, y_test, class_names):\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, y_pred_classes)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(y_test, y_pred_classes, average='weighted')\n",
    "    \n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1-Score: {f1:.4f}\")\n",
    "    \n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred_classes)\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title('Model - Confusion Matrix')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('confusion_matrix.png', dpi=300)\n",
    "    plt.show()\n",
    "    \n",
    "    # classification_report\n",
    "    print(\"\\nDetailed Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred_classes, target_names=class_names))\n",
    "\n",
    "def plot_history(history):\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Accuracy\n",
    "    axs[0].plot(history.history['accuracy'], label='Train Accuracy')\n",
    "    axs[0].plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    axs[0].set_title('Model Accuracy')\n",
    "    axs[0].set_ylabel('Accuracy')\n",
    "    axs[0].set_xlabel('Epoch')\n",
    "    axs[0].legend()\n",
    "    axs[0].grid(True)\n",
    "    \n",
    "    # Loss\n",
    "    axs[1].plot(history.history['loss'], label='Train Loss')\n",
    "    axs[1].plot(history.history['val_loss'], label='Validation Loss')\n",
    "    axs[1].set_title('Model Loss')\n",
    "    axs[1].set_ylabel('Loss')\n",
    "    axs[1].set_xlabel('Epoch')\n",
    "    axs[1].legend()\n",
    "    axs[1].grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('training_history.png', dpi=300)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "121a498a-a17a-42dc-8adc-d9d2540b8afe",
   "metadata": {},
   "source": [
    "## Main function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f942991d-9d94-46f6-b3f7-86a1387de238",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    model, history, X_test, y_test, test_paths = train_model()    \n",
    "\n",
    "    plot_history(history)    \n",
    "\n",
    "    class_names = list(CLASS_MAPPING.keys())\n",
    "    evaluate_model(model, X_test, y_test, class_names)\n",
    "    \n",
    "    # save model\n",
    "    model.save('audio_classification_model.h5')\n",
    "    print(\"mproved model saved!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
